# ANN 2021

## Milestones

  * Week 1 (July 12): Orientation and Intro to ANN
    * (Read Bishop 1-1.1)
    * Course orientation, Introduction to ANN, Colab, Autolab
    * Numerical computation with Numpy: 
      * array: np.array
      * dot operator: np.dot
      * +, - operators: +, -
      * transpose: .T
      * shape: .shape
      * reshape: .reshape
      * matmult: np.matmult, *
      * array to list: .tolist()
      * indexing: x[0][1], x[0,1]
      * slicing: x[0][1:], x[0,1:]
      * vectorization: linspace and np.dot
    * Graphics with Matplotlib
      * plot
      * plot attributes
        * line styles
        * axis labels
        * line colors
      * multiple plots
      * subplots
      * 3D plots

  * Week 2 (July 19): Sweet treat 1: Iris
    * (Read Bishop 1.2-1.2.3)
    * This week we play around data and build a model to achieve a task (predicting its species in this case)
    * Don't worry about theory behind it. We play first, learn along the way, and dig into the theory later. (Remind me too. Sometimes, I get carried away and go too rigorous too early.)
    * Autolab, Iris, linear model w/ manual param finding

  * Week 3 (July 26): National holiday/No class
    * {Read Adv 2.1-2.3}
    
  * Week 4 (Aug 2): Optimization
    * (Read Bishop 1.3)
    * Basic optimization, inc. notation, min/max problem, unconstrained/constrained problems, terminology (e.g., control variables, objective function), min-max transformation, global/local minimizers, and gradient descend method.
    
  * Week 5 (Aug 9): Gradient Descend Method
    * (Read Bishop 1.4)
    * Gradient Descend Behaviors and Working with Dimensionality
      * Effect of initial points
      * Effect of step sizes and # iterations
      * Working with dimensionality
        * 2D problem
        * have fun with 3D plot: surface plot and contour plot
        * optimizing 2D problem
        * run GD on 2D problem
        * remarking on a difficult problem

  * Week 6 (Aug 16): Curve fitting (ML 101)
    * Assignment 1: custom-made model on data of choice (regression is suggested)
    * ML 101: Curve Fitting, Modeling, Model Complexity, SSE, Random Initialization, Training Model
    
  * Week 7 (Aug 23): No class
    * (Read Bishop 5.1)
    
  * Week 8 (Aug 30): Generalization & Model Selection
    * (Read Bishop 5.2)
    * Assignment is due on Sep 4th (Sat), 2021
    * overfitting
    * generalization
    * effects of a large dataset, model complexity and regularization (weight decay) w.r.t. overfitting
    * data separation: train/test
    * model selection and evaluation: train/validation/test
    * performance metrics: SSE and MSE

  * **Week 9: KKU MTE Session: Sep 6-10th**
    * Our MTE will be on Tue 7th Sep 1p-4p
  
  * Week 10 (Sep 13): Perceptron
    * the curse of dimensionality
    * brain and neuron 101
    * development and rise of perceptron
    * fatal criticism on perceptron, decline of ANN popularity and AI winter
  
  * Week 11 (Sep 20): MLP
    * (Read Bishop 5.3)
    * XOR problem
    * Development from perceptron to MLP
    * How difficulty in MLP training has been overcome: reformulation (changing perspective on the problem) and let go of the hard-limit activation function
    * How MLP fits in our theme of modeling: train = min loss and we have done the gradient of the 2-layer MLP

  * Week 12 (Sep 27): MLP Training and coding the MLP
    * {Read Adv 3.4-3.5}
    * Vectorizing the gradients of two-layer MLPs
    * Coding the MLP and MLP training
    * Seeing MLP run for the entire process

  * Week 13 (Oct 4): Applications of ANN (normalization, classification, practical issues)
    * Assignment 2: 2-layer MLP on MNIST
    * Intuition how MLP works
    * Revisit MLP on XOR
    * Applications of MLP to classifications
    * Also, we have seen some numerical issues and how to overcome them.
    
  * Week 14 (Oct 11): Application examples: Yacht Hydrodynamics
    * Practical issues, e.g., normalization, early stopping
    * Batch/online learning and mini batch
    * Bias-variance dilemma
    * Learning curve
    * Regression example: Yacht Hydrodynamics    
    
  * Week 15 (Oct 18): Application examples: Mammographic Mass
    * (Read {Adv 1.4 (MLs), Adv Ch. 5 (Deep Learning)} and Goodfellow Part 3 (DL research))
    * Binary classification example: Mammographic Mass
    * cancer 101
    * have "the end" in mind
    * value the value, not popularity
    * brainstorming, leadership, and importance of team building
    * coding input
    * handling missing data
    * F-score
    * handling unbalanced data

  * Week 16 (Oct 25): Application examples: MNIST & Beyond 2-layer MLP
    * (Read Yolo paper, CV domain)
    * Multi-class classification example: MNIST
    * Beyond 2-layer MLP: supervised learning, unsupervised learning, and others (e.g., semi-supervised learning, collaborative filtering, representation learning, reinforcement learning).

  * Week 17 (Nov 1): Selected topic / advanced discussion / mediapipe / paper discussion
    * Beyond 2-layer MLP: deep learning
    * Paper discussion
      * 1 What have you learned?
      * 2 What do you like about this paper?
      * 3 What makes you curious most after reading the paper?
      * 4 What do you not like about it or you think it could be done better or done another way?
      * 5 What is the main point of the paper? (Key contribution)
      * 6 What does the paper provide to support its main point?
      * 7 What is the limitation of argument the paper provides? (What could weaken its point?) 
      
* **Weeks 18-20: KKU FE Session: Nov 8th-27th**

## (future class?)
Read paper of choice
(selected from ICLR, NIPS, CVPR, ACL or instructor's consent)

paperwithcode
  * key contributions/creative points
  * domain/background
  * problem/solution
  * evaluation/result

## Tricks

Write math in the notebook
```
from IPython.display import Markdown, display
def printmd(string):
    display(Markdown(string))

printmd('**THE EQUATION:**')
printmd(r'$$\int_{{t=0}}^1 \frac{{1}}{{1+t^2}}\,\text{d}t$$')
```

## Datasets

  * [Heart Disease UCI](https://www.kaggle.com/ronitf/heart-disease-uci)
  * [WHO Life Expectancy](https://www.kaggle.com/kumarajarshi/life-expectancy-who)
  * [Tesla Stock 2010-2021](https://www.kaggle.com/abhimaneukj/tesla-inc-tsla-dataset)
  * [Goodreads Books](https://www.kaggle.com/jealousleopard/goodreadsbooks)
  * [WikiHow Summarization](https://www.kaggle.com/varunucl/wikihow-summarization)
  * [ArXiv](https://www.kaggle.com/Cornell-University/arxiv)
  * [COVID-19 Research Challege](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)
  * [Stanford Question Answering](https://www.kaggle.com/stanfordu/stanford-question-answering-dataset)
  * [Wikibooks](https://www.kaggle.com/dhruvildave/wikibooks-dataset/)
  * [Credit Card Approval Prediction](https://www.kaggle.com/rikdifos/credit-card-approval-prediction)
  * [Loan Prediction](https://www.kaggle.com/subhamjain/loan-prediction-based-on-customer-behavior)

## Archive

* Class 1: Introduction to ANN (Done. COE & DME)
  * what is ANN?
  * what is ANN in relation to AI, datamining, data science, deep learning, and big data.
  * course orientation
  * numerical programming

* Class 2: Sweet treat: Iris classification (Done. COE & DME)
  * loading data
  * visualization
  * sklearn
  * Iris data
  * binary classification with a **naive linear model**
    * have students head up for math modeling
    * a relation between prediction behavior and model parameters

* Class 3: Introduction to Optimization (Done. COE & DME)
  * what is optimization?
  * notation and terminology
  * global vs local minimizer
  * minimization vs maximization
  * gradient descend algorithm
    * intuition
    * algorithm
    * tracking progress (leftover DME, Aug 4th)
  
* Class 4: Gradient Descend Algorithm
  * step size and # iterations
  * initial values
  * terminating condition
  * multidimensional space

* Class 5: Polynomial Curve Fitting

* Class 6: Model Selection

* Class 7: Perceptron

* Class 8: Two-Layer MLP

* Class 9: Training MLP

* Class 10: Error Backpropagation

* Class 11: MLP applications
  * regression
  * binary classification
  * multi-class classification
  
* Class 12: Introduction to Deep Learning
  * vanishing gradient problem
  * ReLu
  * Key factors, e.g., CNN, pre-training, Batchnorm, advanced training alogirithms, Xavier initialization 
  * other factors, e.g., RNN, lstm, gru, GAN, attention, inception model, resnet, densenet, transformer, GPT-3, etc.

* Class 13: Selected application/paper reading
  * E.g., YOLO 

* Class Project

---

# Learn and to be improved for the next time

  * Cut down the assignment and report: they are burden!!! I won't have time to properly read, grade or comment them 
    * otherwise, have a grading session that students help grade each other and learn how to write good reports in the same time. 
    * ASSIGNMENT WITH CLASS GRADING SESSION: hand out students each a report of other, then show they how to grade, let them grade, hand it back; Let them have chance to fix, do it again and collect the grades this time.
      * Criteria: 
        * Readability 20%
        * Correctness 20%: how data is prepared, how model is prepared, how evaluation is done        
        * Completeness 10%: reproducibility
        * Intellectuality 10%: discussion and conclusion
        * Interestingness 10%: dataset method
        * Overall being 30%
      * Dateset: for student using the same dataset, the best report of the set stays in the same dataset, the others recorrect their reports with other datasets
      
  * I should suppress ```print``` on calling student's submission (See Jinnawat's 1st submission): student can exploit it to reveal what should not be seen!!!

```python
import sys, os

# Disable
def blockPrint():
    sys.stdout = open(os.devnull, 'w')

# Restore
def enablePrint():
    sys.stdout = sys.__stdout__


print 'This will print'

blockPrint()
print "This won't"

enablePrint()
print "This will too"
```
## Report writing

General feedback for Assignment 2 (On-going message. I'll keep updating until the A2 grading is done.)
I think I summarize what you could learn from Assignment 2 and report writing (and experimental study) in general here.
(Sorry that this comes late. I hardly catch up with all the workload I have in hand.)

1. Ask yourself what your report is telling. What are you trying to communicate. This implicit question has to be clear. Your report has to be sufficiently answer it.
--> The goal: to study effects of #HUs and training variables, i.e., lr and #epochs on final performance.
= Question: What are the relationships among #HUs, training variables, and the final performance

2. Roles of each part
* Title, Name and student ids: Of course, you need this.
* Introduction/overview and main goal: This is to give a context, what you are studying, why it matters, and how you will study it.
* Methodology: you elaborate on how you will study it, e.g., your details on the experimental design.
What factors are studied, what factors are controlled, what will be measured, what will be the final index, how the results will be interpreted.
** A nicer written lays big picture, before jumping into the details.
* Model and Hyperparameters: this is specific to our study. Clearly state what model and hyper parameters you are studying.
* Results: you present your experimental results here. It can be presented in many levels.
E.g., 
Lowest level: you can present raw results—test accuracies from each repeat.
Higher level: because OUR GOAL is to study the relation between factors and final performance,
 You can assemble your raw results to produce a plot between, say, HUs and accuracies.
With 10 repeats, you may need box plots or mean to help clarify the general trend.

Once you have evidence sufficiently to answer the QUESTION (the main goal),
you interpret the results using your results to support your arguments.

* Discussion: what you have learned and any interesting points you want to bring up. You discuss what you have learned and you are free to discuss any other related issues that might not cover in your goal here. You can discuss the limitation of your experiments and its ramification on your interpretation and answers. You can discuss potential of what you have found. You speculate another scenario and what would happen. You can put what you have learned in a new perspective. You can compare or contrast the concept with others. Induction. Deduction.Try to gain insight. Try to understand. See it in a big picture. Play around. Try to invoke the intellectual discussion.

* Conclusion: a concise version of what answers you come out in the results. You should not make a new point here. (You can make a new point in discussion. That’s where freely discussed arguments belong.)

***Another key point is that if what you are studying involves randomness, use statistics.***

General assessment of the article (not only for this assignment)
  * readability: english and how you write a proper report. If a reader cannot read your writing, they cannot learn what you try to communicate.
  * Originality: this the freshness of the idea. (It’s the fresh idea—not just “new”, the idea has to be new. It must show surprise, creativity, and ingenuity.)
  * Value or contribution: what has been learned in this article, what knowledge a reader will gain after reading it. Some even says it has to change a reader’s idea.
  * Completeness: for student's report, this is to emphasize that a student has provided all the neccessary parts.
    * for a general article, this could mean adequately providing context (inc. background, related works and their relations to the article).   
  * Correctness: we have a question (main goal). Do you use a sound approach to answer it. E.g., you design a proper experiment to answer it. You interpret the results correctly. This includes using proper statistics as it fits.
  * Overall being: this emcompasses everything, the language, paper structure, each little details, the arguments, the figures, how the entire article looks (does it look like pieces roughly stitched together or does it look like a mindfully written article?), everything.

---

Common comments on many students’ reports

The real missing key here is the relationship between the "decision index" and "our factor of interest", which directly answers the question we are asking.
