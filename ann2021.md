# ANN 2021

## Milestones

  * Week 1 (July 12): Orientation and Intro to ANN
    * (Read Bishop 1-1.1)
    * Course orientation, Introduction to ANN, Colab, Autolab
    * Numerical computation with Numpy: 
      * array: np.array
      * dot operator: np.dot
      * +, - operators: +, -
      * transpose: .T
      * shape: .shape
      * reshape: .reshape
      * matmult: np.matmult, *
      * array to list: .tolist()
      * indexing: x[0][1], x[0,1]
      * slicing: x[0][1:], x[0,1:]
      * vectorization: linspace and np.dot
    * Graphics with Matplotlib
      * plot
      * plot attributes
        * line styles
        * axis labels
        * line colors
      * multiple plots
      * subplots
      * 3D plots

  * Week 2 (July 19): Sweet treat 1: Iris
    * (Read Bishop 1.2-1.2.3)
    * This week we play around data and build a model to achieve a task (predicting its species in this case)
    * Don't worry about theory behind it. We play first, learn along the way, and dig into the theory later. (Remind me too. Sometimes, I get carried away and go too rigorous too early.)
    * Autolab, Iris, linear model w/ manual param finding

  * Week 3 (July 26): National holiday/No class
    * {Read Adv 2.1-2.3}
    
  * Week 4 (Aug 2): Optimization
    * (Read Bishop 1.3)
    * Basic optimization, inc. notation, min/max problem, unconstrained/constrained problems, terminology (e.g., control variables, objective function), min-max transformation, global/local minimizers, and gradient descend method.
    
  * Week 5 (Aug 9): Gradient Descend Method
    * (Read Bishop 1.4)
    * Gradient Descend Behaviors and Working with Dimensionality
      * Effect of initial points
      * Effect of step sizes and # iterations
      * Working with dimensionality
        * 2D problem
        * have fun with 3D plot: surface plot and contour plot
        * optimizing 2D problem
        * run GD on 2D problem
        * remarking on a difficult problem

  * Week 6 (Aug 16): Curve fitting (ML 101)
    * Assignment 1: custom-made model on data of choice (regression is suggested)
    * ML 101: Curve Fitting, Modeling, Model Complexity, SSE, Random Initialization, Training Model
    
  * Week 7 (Aug 23): No class
    * (Read Bishop 5.1)
    
  * Week 8 (Aug 30): Generalization & Model Selection
    * (Read Bishop 5.2)
    * Assignment is due on Sep 4th (Sat), 2021
    * overfitting
    * generalization
    * effects of a large dataset, model complexity and regularization (weight decay) w.r.t. overfitting
    * data separation: train/test
    * model selection and evaluation: train/validation/test
    * performance metrics: SSE and MSE

  * **Week 9: KKU MTE Session: Sep 6-10th**
    * Our MTE will be on Tue 7th Sep 1p-4p
  
  * Week 10 (Sep 13): Perceptron
    * the curse of dimensionality
    * brain and neuron 101
    * development and rise of perceptron
    * fatal criticism on perceptron, decline of ANN popularity and AI winter
  
  * Week 11 (Sep 20): MLP
    * (Read Bishop 5.3)
    * XOR problem
    * Development from perceptron to MLP
    * How difficulty in MLP training has been overcome: reformulation (changing perspective on the problem) and let go of the hard-limit activation function
    * How MLP fits in our theme of modeling: train = min loss and we have done the gradient of the 2-layer MLP

  * Week 12 (Sep 27): MLP Training and coding the MLP
    * {Read Adv 3.4-3.5}
    * Vectorizing the gradients of two-layer MLPs
    * Coding the MLP and MLP training
    * Seeing MLP run for the entire process

  * Week 13 (Oct 4): Applications of ANN (normalization, classification, practical issues)
    * Assignment 2: 2-layer MLP on MNIST
    * Intuition how MLP works
    * Revisit MLP on XOR
    * Applications of MLP to classifications
    * Also, we have seen some numerical issues and how to overcome them.
    
  * Week 14 (Oct 11): Application examples: Yacht Hydrodynamics
    * Practical issues, e.g., normalization, early stopping
    * Batch/online learning and mini batch
    * Bias-variance dilemma
    * Learning curve
    * Regression example: Yacht Hydrodynamics    
    
  * Week 15 (Oct 18): Application examples: Mammographic Mass
    * (Read {Adv 1.4 (MLs), Adv Ch. 5 (Deep Learning)} and Goodfellow Part 3 (DL research))
    * Binary classification example: Mammographic Mass
    * cancer 101
    * have "the end" in mind
    * value the value, not popularity
    * brainstorming, leadership, and importance of team building
    * coding input
    * handling missing data
    * F-score
    * handling unbalanced data

  * Week 16 (Oct 25): Application examples: MNIST & Beyond 2-layer MLP
    * (Read Yolo paper, CV domain)
    * Multi-class classification example: MNIST
    * Beyond 2-layer MLP: supervised learning, unsupervised learning, and others (e.g., semi-supervised learning, collaborative filtering, representation learning, reinforcement learning).

  * Week 17 (Nov 1): Selected topic / advanced discussion / mediapipe / paper discussion
    * Beyond 2-layer MLP: deep learning
    * Paper discussion
      * 1 What have you learned?
      * 2 What do you like about this paper?
      * 3 What makes you curious most after reading the paper?
      * 4 What do you not like about it or you think it could be done better or done another way?
      * 5 What is the main point of the paper? (Key contribution)
      * 6 What does the paper provide to support its main point?
      * 7 What is the limitation of argument the paper provides? (What could weaken its point?) 
      
* **Weeks 18-20: KKU FE Session: Nov 8th-27th**

## (future class?)
Read paper of choice
(selected from ICLR, NIPS, CVPR, ACL or instructor's consent)

paperwithcode
  * key contributions/creative points
  * domain/background
  * problem/solution
  * evaluation/result

## Tricks

Write math in the notebook
```
from IPython.display import Markdown, display
def printmd(string):
    display(Markdown(string))

printmd('**THE EQUATION:**')
printmd(r'$$\int_{{t=0}}^1 \frac{{1}}{{1+t^2}}\,\text{d}t$$')
```

## Datasets

  * [Heart Disease UCI](https://www.kaggle.com/ronitf/heart-disease-uci)
  * [WHO Life Expectancy](https://www.kaggle.com/kumarajarshi/life-expectancy-who)
  * [Tesla Stock 2010-2021](https://www.kaggle.com/abhimaneukj/tesla-inc-tsla-dataset)
  * [Goodreads Books](https://www.kaggle.com/jealousleopard/goodreadsbooks)
  * [WikiHow Summarization](https://www.kaggle.com/varunucl/wikihow-summarization)
  * [ArXiv](https://www.kaggle.com/Cornell-University/arxiv)
  * [COVID-19 Research Challege](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)
  * [Stanford Question Answering](https://www.kaggle.com/stanfordu/stanford-question-answering-dataset)
  * [Wikibooks](https://www.kaggle.com/dhruvildave/wikibooks-dataset/)
  * [Credit Card Approval Prediction](https://www.kaggle.com/rikdifos/credit-card-approval-prediction)
  * [Loan Prediction](https://www.kaggle.com/subhamjain/loan-prediction-based-on-customer-behavior)

## Archive

* Class 1: Introduction to ANN (Done. COE & DME)
  * what is ANN?
  * what is ANN in relation to AI, datamining, data science, deep learning, and big data.
  * course orientation
  * numerical programming

* Class 2: Sweet treat: Iris classification (Done. COE & DME)
  * loading data
  * visualization
  * sklearn
  * Iris data
  * binary classification with a **naive linear model**
    * have students head up for math modeling
    * a relation between prediction behavior and model parameters

* Class 3: Introduction to Optimization (Done. COE & DME)
  * what is optimization?
  * notation and terminology
  * global vs local minimizer
  * minimization vs maximization
  * gradient descend algorithm
    * intuition
    * algorithm
    * tracking progress (leftover DME, Aug 4th)
  
* Class 4: Gradient Descend Algorithm
  * step size and # iterations
  * initial values
  * terminating condition
  * multidimensional space

* Class 5: Polynomial Curve Fitting

* Class 6: Model Selection

* Class 7: Perceptron

* Class 8: Two-Layer MLP

* Class 9: Training MLP

* Class 10: Error Backpropagation

* Class 11: MLP applications
  * regression
  * binary classification
  * multi-class classification
  
* Class 12: Introduction to Deep Learning
  * vanishing gradient problem
  * ReLu
  * Key factors, e.g., CNN, pre-training, Batchnorm, advanced training alogirithms, Xavier initialization 
  * other factors, e.g., RNN, lstm, gru, GAN, attention, inception model, resnet, densenet, transformer, GPT-3, etc.

* Class 13: Selected application/paper reading
  * E.g., YOLO 

* Class Project

---

# Learn

  * Cut down the assignment and report: they are burden!!! I won't have time to properly read, grade or comment them 
    * otherwise, have a grading session that students help grade each other and learn how to write good reports in the same time. 
    * ASSIGNMENT WITH CLASS GRADING SESSION: hand out students each a report of other, then show they how to grade, let them grade, hand it back; Let them have chance to fix, do it again and collect the grades this time.
      * Criteria: 
        * Readability 20%
        * Correctness 20%: how data is prepared, how model is prepared, how evaluation is done        
        * Completeness 10%: reproducibility
        * Intellectuality 10%: discussion and conclusion
        * Interestingness 10%: dataset method
        * Overall being 30%
  * I should suppress ```print``` on calling student's submission (See Jinnawat's 1st submission): student can exploit it to reveal what should not be seen!!!

```python
import sys, os

# Disable
def blockPrint():
    sys.stdout = open(os.devnull, 'w')

# Restore
def enablePrint():
    sys.stdout = sys.__stdout__


print 'This will print'

blockPrint()
print "This won't"

enablePrint()
print "This will too"
```
